{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import BartForSequenceClassification, BartForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.utils import PaddingStrategy\n",
    "import evaluate\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from datasets import ClassLabel\n",
    "from transformers import DataCollatorWithPadding\n",
    "from itertools import tee, chain\n",
    "from functools import partial\n",
    "from pprint import pformat\n",
    "\n",
    "PRETRAINED_MODEL_NAME_OR_PATH=\"ainize/bart-base-cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def setup_models():\n",
    "    # initialize models\n",
    "    classification_model = BartForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH, num_labels=59)\n",
    "    summarization_model = BartForConditionalGeneration.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "\n",
    "    # share parameters\n",
    "    summarization_model.model.shared = classification_model.model.shared\n",
    "    summarization_model.model.encoder = classification_model.model.encoder\n",
    "    summarization_model.model.decoder = classification_model.model.decoder\n",
    "\n",
    "    return {\n",
    "        \"summarization\": summarization_model,\n",
    "        \"classification\": classification_model\n",
    "    }\n",
    "\n",
    "def preprocess_docee(examples, tokenizer):\n",
    "    batch_encoding = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "    batch_encoding[\"labels\"] = examples[\"event_type\"]\n",
    "    return batch_encoding\n",
    "\n",
    "def print_first_param(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name} is {param}\")\n",
    "        break\n",
    "\n",
    "def compose2(f, g):\n",
    "    def composition(*args, **kwargs):\n",
    "        g_output = g(*args, **kwargs)\n",
    "        return f(g_output)\n",
    "    return composition\n",
    "\n",
    "def c(*fs):\n",
    "    def composition(*args, **kwargs):\n",
    "        output = fs[-1](*args, **kwargs)\n",
    "        for f in reversed(fs[:-1]):\n",
    "            output = f(output)\n",
    "        return output\n",
    "    return composition\n",
    "\n",
    "def process_summary_example(\n",
    "        examples,\n",
    "        tokenizer,\n",
    "        max_input_length=1024,\n",
    "        max_target_length=100\n",
    "):\n",
    "    # tokenize the article\n",
    "    batch_encoding = tokenizer(\n",
    "        examples[\"article\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # tokenize the labels\n",
    "    tokenized_highlights = tokenizer(\n",
    "        examples[\"highlights\"],\n",
    "        max_length=max_target_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    batch_encoding[\"labels\"] = tokenized_highlights[\"input_ids\"]\n",
    "    return batch_encoding\n",
    "\n",
    "def setup_tasks(task_templates, models, num_epochs=1):\n",
    "\n",
    "    tasks = {\n",
    "        task: {\n",
    "            \"model\": models[task],\n",
    "            \"optimizer\": None,\n",
    "            \"train_dataloader\": DataLoader(\n",
    "                stuff[\"train_dataset\"] ,\n",
    "                batch_size=stuff[\"batch_size\"],\n",
    "                shuffle=True,\n",
    "                collate_fn=stuff[\"collate_fn\"],\n",
    "                pin_memory=True\n",
    "            ),\n",
    "            \"eval_dataloader\": DataLoader(\n",
    "                stuff[\"eval_dataset\"],\n",
    "                batch_size=stuff[\"batch_size\"],\n",
    "                collate_fn=stuff[\"collate_fn\"],\n",
    "                pin_memory=True\n",
    "            )\n",
    "\n",
    "        }\n",
    "        for task, stuff in task_templates.items()\n",
    "    }\n",
    "\n",
    "    setup_optimizers(tasks)\n",
    "    accelerate(tasks)\n",
    "    summ_cls_ratio = len(tasks[\"summarization\"][\"train_dataloader\"]) // len(tasks[\"classification\"][\"train_dataloader\"]) + 1\n",
    "    setup_schedulers(tasks, num_epochs=num_epochs, summ_cls_ratio=summ_cls_ratio)\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def compute_rouge(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def get_param_list(model):\n",
    "    return [\n",
    "               param for param in model.shared.parameters()\n",
    "           ] + [\n",
    "               param for param in model.encoder.parameters()\n",
    "           ] + [\n",
    "               param for param in model.decoder.parameters()\n",
    "           ]\n",
    "\n",
    "\n",
    "def setup_optimizers(tasks):\n",
    "    for task_name, task_objects in tasks.items():\n",
    "        print(f\"Setting up {task_name}\")\n",
    "        task_objects[\"optimizer\"] = AdamW(task_objects[\"model\"].parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "def accelerate(tasks):\n",
    "    for task in tasks:\n",
    "        accelerator = Accelerator()\n",
    "        # tasks[task][\"accelerator\"] = Accelerator()\n",
    "        for component in [\"model\", \"optimizer\", \"train_dataloader\", \"eval_dataloader\"]:\n",
    "            tasks[task][component] = accelerator.prepare(tasks[task][component])\n",
    "        tasks[task][\"accelerator\"] = accelerator\n",
    "\n",
    "\n",
    "def setup_schedulers(tasks, num_epochs, summ_cls_ratio):\n",
    "    tasks[\"summarization\"][\"lr_scheduler\"] = get_scheduler(\n",
    "        \"linear\",\n",
    "        tasks[\"summarization\"][\"optimizer\"],\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_epochs * len(tasks[\"summarization\"][\"train_dataloader\"])\n",
    "    )\n",
    "    tasks[\"classification\"][\"lr_scheduler\"] = get_scheduler(\n",
    "        \"linear\",\n",
    "        tasks[\"classification\"][\"optimizer\"],\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_epochs * len(tasks[\"classification\"][\"train_dataloader\"]) * summ_cls_ratio\n",
    "    )\n",
    "\n",
    "\n",
    "def train(tasks, num_epochs, tokenizer):\n",
    "    rouge_score = evaluate.load(\"rouge\")\n",
    "    classification_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epoch\", total=num_epochs):\n",
    "\n",
    "        summ_cls_ratio = len(tasks[\"summarization\"][\"train_dataloader\"]) // len(tasks[\"classification\"][\"train_dataloader\"]) + 1\n",
    "        print(f\"{summ_cls_ratio = }\")\n",
    "\n",
    "        print(f\"Dataloader for classification will be replicated {summ_cls_ratio} times.\")\n",
    "        cls_tdl_len = len(tasks[\"classification\"][\"train_dataloader\"])\n",
    "        print(f\"Instead of {cls_tdl_len}, classification dataset iterator will yield {cls_tdl_len * summ_cls_ratio} examples.\")\n",
    "\n",
    "        # load training data, step by step\n",
    "        num_epoch_steps = len(tasks[\"summarization\"][\"train_dataloader\"]) * 2\n",
    "        iters = {task: iter(tasks[task][\"train_dataloader\"]) for task in tasks}\n",
    "        iters[\"classification\"] = chain(*tee(iter(tasks[\"classification\"][\"train_dataloader\"]), summ_cls_ratio))\n",
    "\n",
    "        tasks[\"classification\"][\"train_len\"] = cls_tdl_len * summ_cls_ratio\n",
    "        tasks[\"summarization\"][\"train_len\"] = len(tasks[\"summarization\"][\"train_dataloader\"])\n",
    "\n",
    "        progress_bars = {\n",
    "            task: tqdm(range(tasks[task][\"train_len\"]), desc=f\"{task} progress in epoch {epoch+1}\", total=tasks[task][\"train_len\"], leave=False)\n",
    "            for task in tasks\n",
    "        }\n",
    "\n",
    "        set_train(tasks)\n",
    "        for step in range(num_epoch_steps):\n",
    "            if step % 2 == 0: # train summarization\n",
    "                task = \"summarization\"\n",
    "            else:\n",
    "                task = \"classification\"\n",
    "            batch = next(iters[task])\n",
    "            with tasks[task][\"accelerator\"].accumulate(tasks[task][\"model\"]):\n",
    "                outputs = tasks[task][\"model\"](**batch)\n",
    "                loss = outputs.loss\n",
    "                tasks[task][\"accelerator\"].backward(loss)\n",
    "                tasks[task][\"optimizer\"].step()\n",
    "                tasks[task][\"lr_scheduler\"].step()\n",
    "                tasks[task][\"optimizer\"].zero_grad()\n",
    "                progress_bars[task].update(1)\n",
    "\n",
    "        # TODO - gradient accumulation\n",
    "        # TODO - loss weighing\n",
    "\n",
    "        # idea -> instead of alternating batches, we could scale losses\n",
    "        # idea2 -> GAN setup?\n",
    "        #   -> generator tries to generate summaries\n",
    "        #   -> discriminator predicts event types base on those summaries\n",
    "        #   -> generator wants to generate such that discriminator is able to predict labels easier\n",
    "        #\n",
    "        # this would also be expensive AS FUCK to train\n",
    "        #\n",
    "        # would this work, and why not?\n",
    "        #   where are the real/fake examples?\n",
    "        #\n",
    "\n",
    "        # evaluation at the end of epoch\n",
    "        set_eval(tasks)\n",
    "\n",
    "        # evaluate summarization\n",
    "        accelerator = tasks[\"summarization\"][\"accelerator\"]\n",
    "        for step, batch in enumerate(tasks[\"summarization\"][\"eval_dataloader\"]):\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = accelerator.unwrap_model(tasks[\"summarization\"][\"model\"]).generate(\n",
    "                    batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                )  # aha! we can plug the generation parameters here\n",
    "\n",
    "                generated_tokens = accelerator.pad_across_processes(\n",
    "                    generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "                )\n",
    "                labels = batch[\"labels\"]\n",
    "\n",
    "                # If we did not pad to max length, we need to pad the labels too\n",
    "                labels = accelerator.pad_across_processes(\n",
    "                    labels, dim=1, pad_index=tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "                generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "                labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "                # Replace -100 in the labels as we can't decode them\n",
    "                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "                if isinstance(generated_tokens, tuple):\n",
    "                    generated_tokens = generated_tokens[0]\n",
    "                decoded_preds = tokenizer.batch_decode(\n",
    "                    generated_tokens, skip_special_tokens=True\n",
    "                )\n",
    "                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "                decoded_preds, decoded_labels = postprocess_text(\n",
    "                    decoded_preds, decoded_labels\n",
    "                )\n",
    "\n",
    "                rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "                # evaluation loop is fine for summarization but we need it for classification as well\n",
    "\n",
    "        # Compute metrics\n",
    "        result = rouge_score.compute()\n",
    "        # Extract the median ROUGE scores\n",
    "        result = {key: value * 100 for key, value in result.items()}\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "        print(f\"[SUMM] Epoch {epoch+1}:\", pformat(result))\n",
    "\n",
    "        output_dir = f\"./test_mtl/summ_epoch_{epoch+1}\"\n",
    "        # Save and upload\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(tasks[\"summarization\"][\"model\"])\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(output_dir)  # ovo treba samo jednom realno\n",
    "\n",
    "\n",
    "        # evaluate classification\n",
    "        eval_dataloader = tasks[\"classification\"][\"eval_dataloader\"]\n",
    "        accelerator = tasks[\"classification\"][\"accelerator\"]\n",
    "        model = tasks[\"classification\"][\"model\"]\n",
    "        for batch in tqdm(eval_dataloader, total=len(eval_dataloader), desc=\"[CLS] Evaluation\"):\n",
    "            # extract outputs\n",
    "            outputs = accelerator.unwrap_model(model)(**batch)\n",
    "            # print(outputs.keys())  # loss, logits, encoder_last_hidden_state\n",
    "\n",
    "            # outputs[\"logits\"] = (BS, 59)\n",
    "            # we need argmax by dimension 1\n",
    "\n",
    "            # decode logits into labels\n",
    "            predictions = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "            predictions = accelerator.gather(predictions).cpu().numpy()\n",
    "            # print(labels)\n",
    "            classification_f1.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=batch[\"labels\"].cpu().numpy(),\n",
    "            )\n",
    "            # break\n",
    "            # f1.add_batch(predictions=outputs[\"labels\"])\n",
    "        result = classification_f1.compute(average=\"macro\")\n",
    "        print(f\"[CLS] Epoch {epoch+1}: {pformat(result)}\")\n",
    "\n",
    "        output_dir = f\"./test_mtl/cls_epoch_{epoch+1}\"\n",
    "        # Save and upload\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "\n",
    "        # pa ovo radi, bruhus maximus\n",
    "\n",
    "\n",
    "def setup_dummy_dataset(cls_train_size, cls_eval_size, summ_train_size, summ_eval_size, tokenizer):\n",
    "    summ = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    cls = load_dataset(\"csv\", data_files={\n",
    "        \"train\": \"../data/docee/18091999/train.csv\",\n",
    "        \"validation\": \"../data/docee/18091999/early_stopping.csv\"\n",
    "    })\n",
    "    event_names = cls[\"train\"].unique(\"event_type\")\n",
    "    cls = cls.cast_column(\"event_type\", ClassLabel(num_classes=len(event_names), names=sorted(event_names)))\n",
    "\n",
    "    def setup_dataset_split(dataset, split, n_examples, preprocessing):\n",
    "        return dataset[split].shuffle().select(range(n_examples)).map(preprocessing, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "    setup_cls = partial(setup_dataset_split, dataset=cls, preprocessing=partial(preprocess_docee, tokenizer=tokenizer))\n",
    "    setup_summ = partial(setup_dataset_split, dataset=summ, preprocessing=partial(process_summary_example, tokenizer=tokenizer))\n",
    "\n",
    "    cls_train = setup_cls(split=\"train\", n_examples=cls_train_size)\n",
    "    cls_eval = setup_cls(split=\"validation\", n_examples=cls_eval_size)\n",
    "    summ_train = setup_summ(split=\"train\", n_examples=summ_train_size)\n",
    "    summ_eval = setup_summ(split=\"validation\", n_examples=summ_eval_size)\n",
    "\n",
    "    print(f\"{len(cls_train) = }\")\n",
    "    print(f\"{len(cls_eval) = }\")\n",
    "    print(f\"{len(summ_train) = }\")\n",
    "    print(f\"{len(summ_eval) = }\")\n",
    "\n",
    "    return cls_train, cls_eval, summ_train, summ_eval\n",
    "\n",
    "\n",
    "def set_train(tasks):\n",
    "    for task in tasks:\n",
    "        print(f\"setting {task} to train\")\n",
    "        tasks[task][\"model\"].train()\n",
    "\n",
    "def set_eval(tasks):\n",
    "    for task in tasks:\n",
    "        print(f\"setting {task} to eval\")\n",
    "        tasks[task][\"model\"].eval()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "\n",
    "def pipeline(num_epochs=3):\n",
    "    models = setup_models()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "    cls_train, cls_eval, summ_train, summ_eval = setup_dummy_dataset(\n",
    "        cls_train_size=5,\n",
    "        cls_eval_size=5,\n",
    "        summ_train_size=12,\n",
    "        summ_eval_size=8,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    task_templates = {\n",
    "        \"classification\": {\n",
    "            \"train_dataset\": cls_train,\n",
    "            \"eval_dataset\": cls_eval,\n",
    "            \"batch_size\": 1,\n",
    "            \"collate_fn\": DataCollatorWithPadding(\n",
    "                tokenizer=tokenizer,\n",
    "                padding=PaddingStrategy.MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        },\n",
    "        \"summarization\": {\n",
    "            \"train_dataset\": summ_train,\n",
    "            \"eval_dataset\": summ_eval,\n",
    "            \"batch_size\": 1,\n",
    "            \"collate_fn\": DataCollatorForSeq2Seq(\n",
    "                tokenizer=tokenizer,\n",
    "                padding=PaddingStrategy.MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        },\n",
    "    }\n",
    "    tasks = setup_tasks(task_templates, models, num_epochs=num_epochs)\n",
    "    train(tasks, num_epochs=num_epochs, tokenizer=tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ainize/bart-base-cnn were not used when initializing BartForSequenceClassification: ['final_logits_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at ainize/bart-base-cnn and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset cnn_dailymail (/home/jvidakovic/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd5a207194244571a1d510dec5b3419e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0720af0f377253e9\n",
      "Found cached dataset csv (/home/jvidakovic/.cache/huggingface/datasets/csv/default-0720af0f377253e9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cba071fe2fd46ccb435a1e3b0248533"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jvidakovic/.cache/huggingface/datasets/csv/default-0720af0f377253e9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5f6bf4a5560153b2.arrow\n",
      "Loading cached processed dataset at /home/jvidakovic/.cache/huggingface/datasets/csv/default-0720af0f377253e9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d42b77ea8f3482ef.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f50ce72b9f15438da07c7e8f381f3f19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8baed29a681847bd935df5aa4302f5d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04da121aa35b4d40966dc70ba7a93a7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9149316b10ee4c8996148de6ae335c47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cls_train) = 5\n",
      "len(cls_eval) = 5\n",
      "len(summ_train) = 12\n",
      "len(summ_eval) = 8\n",
      "Setting up classification\n",
      "Setting up summarization\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ced3664087f84065af51ca12cc8379f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summ_cls_ratio = 3\n",
      "Dataloader for classification will be replicated 3 times.\n",
      "Instead of 5, classification dataset iterator will yield 15 examples.\n"
     ]
    },
    {
     "data": {
      "text/plain": "classification progress in epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54222c047c704c628b7c946cc25788d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "summarization progress in epoch 1:   0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4467a1c7a67742418c92dbeedefea12b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting classification to train\n",
      "setting summarization to train\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/tmp/ipykernel_30010/\u001B[0m\u001B[1;33m2513993120.py\u001B[0m:\u001B[94m1\u001B[0m in \u001B[92m<cell line: 1>\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_30010/2513993120.py'\u001B[0m                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/tmp/ipykernel_30010/\u001B[0m\u001B[1;33m533649130.py\u001B[0m:\u001B[94m37\u001B[0m in \u001B[92mpipeline\u001B[0m                                                 \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_30010/533649130.py'\u001B[0m                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/tmp/ipykernel_30010/\u001B[0m\u001B[1;33m270439452.py\u001B[0m:\u001B[94m203\u001B[0m in \u001B[92mtrain\u001B[0m                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_30010/270439452.py'\u001B[0m                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mch/nn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnsformers/models/bart/\u001B[0m\u001B[1;33mmodeling_bart.py\u001B[0m:\u001B[94m1516\u001B[0m in \u001B[92mforward\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1513 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[33mf\u001B[0m\u001B[33m\"\u001B[0m\u001B[33mPassing input embeddings is currently not supported for \u001B[0m\u001B[33m{\u001B[0m\u001B[96mself\u001B[0m.\u001B[91m__class_\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1514 \u001B[0m\u001B[2m│   │   │   \u001B[0m)                                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1515 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1516 \u001B[2m│   │   \u001B[0moutputs = \u001B[96mself\u001B[0m.model(                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1517 \u001B[0m\u001B[2m│   │   │   \u001B[0minput_ids,                                                                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1518 \u001B[0m\u001B[2m│   │   │   \u001B[0mattention_mask=attention_mask,                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1519 \u001B[0m\u001B[2m│   │   │   \u001B[0mdecoder_input_ids=decoder_input_ids,                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mch/nn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnsformers/models/bart/\u001B[0m\u001B[1;33mmodeling_bart.py\u001B[0m:\u001B[94m1251\u001B[0m in \u001B[92mforward\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1248 \u001B[0m\u001B[2m│   │   │   \u001B[0m)                                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1249 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1250 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_att\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1251 \u001B[2m│   │   \u001B[0mdecoder_outputs = \u001B[96mself\u001B[0m.decoder(                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1252 \u001B[0m\u001B[2m│   │   │   \u001B[0minput_ids=decoder_input_ids,                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1253 \u001B[0m\u001B[2m│   │   │   \u001B[0mattention_mask=decoder_attention_mask,                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1254 \u001B[0m\u001B[2m│   │   │   \u001B[0mencoder_hidden_states=encoder_outputs[\u001B[94m0\u001B[0m],                                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mch/nn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnsformers/models/bart/\u001B[0m\u001B[1;33mmodeling_bart.py\u001B[0m:\u001B[94m1107\u001B[0m in \u001B[92mforward\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1104 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m)                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1105 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94melse\u001B[0m:                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1106 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m                                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1107 \u001B[2m│   │   │   │   \u001B[0mlayer_outputs = decoder_layer(                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1108 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mhidden_states,                                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1109 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mattention_mask=attention_mask,                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1110 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mencoder_hidden_states=encoder_hidden_states,                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mch/nn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnsformers/models/bart/\u001B[0m\u001B[1;33mmodeling_bart.py\u001B[0m:\u001B[94m439\u001B[0m in \u001B[92mforward\u001B[0m                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 436 \u001B[0m\u001B[2m│   │   │   \u001B[0m                                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 437 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[2m# cross_attn cached key/values tuple is at positions 3,4 of present_key_valu\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 438 \u001B[0m\u001B[2m│   │   │   \u001B[0mcross_attn_past_key_value = past_key_value[-\u001B[94m2\u001B[0m:] \u001B[94mif\u001B[0m past_key_value \u001B[95mis\u001B[0m \u001B[95mnot\u001B[0m \u001B[94mNon\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 439 \u001B[2m│   │   │   \u001B[0mhidden_states, cross_attn_weights, cross_attn_present_key_value = \u001B[96mself\u001B[0m.encod  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 440 \u001B[0m\u001B[2m│   │   │   │   \u001B[0mhidden_states=hidden_states,                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 441 \u001B[0m\u001B[2m│   │   │   │   \u001B[0mkey_value_states=encoder_hidden_states,                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 442 \u001B[0m\u001B[2m│   │   │   │   \u001B[0mattention_mask=encoder_attention_mask,                                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mch/nn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnsformers/models/bart/\u001B[0m\u001B[1;33mmodeling_bart.py\u001B[0m:\u001B[94m243\u001B[0m in \u001B[92mforward\u001B[0m                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 240 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mraise\u001B[0m \u001B[96mValueError\u001B[0m(                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 241 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0m\u001B[33mf\u001B[0m\u001B[33m\"\u001B[0m\u001B[33mAttention mask should be of size \u001B[0m\u001B[33m{\u001B[0m(bsz, \u001B[94m1\u001B[0m, tgt_len, src_len)\u001B[33m}\u001B[0m\u001B[33m, but\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 242 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m)                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 243 \u001B[2m│   │   │   \u001B[0mattn_weights = attn_weights.view(bsz, \u001B[96mself\u001B[0m.num_heads, tgt_len, src_len) + at  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 244 \u001B[0m\u001B[2m│   │   │   \u001B[0mattn_weights = attn_weights.view(bsz * \u001B[96mself\u001B[0m.num_heads, tgt_len, src_len)      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 245 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 246 \u001B[0m\u001B[2m│   │   \u001B[0mattn_weights = nn.functional.softmax(attn_weights, dim=-\u001B[94m1\u001B[0m)                        \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mRuntimeError: \u001B[0mCUDA out of memory. Tried to allocate \u001B[1;36m48.00\u001B[0m MiB \u001B[1m(\u001B[0mGPU \u001B[1;36m0\u001B[0m; \u001B[1;36m5.93\u001B[0m GiB total capacity; \u001B[1;36m5.26\u001B[0m GiB already \nallocated; \u001B[1;36m25.06\u001B[0m MiB free; \u001B[1;36m5.41\u001B[0m GiB reserved in total by PyTorch\u001B[1m)\u001B[0m If reserved memory is >> allocated memory try \nsetting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_30010/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2513993120.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_30010/2513993120.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_30010/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">533649130.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">37</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pipeline</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_30010/533649130.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_30010/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">270439452.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">203</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_30010/270439452.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nsformers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1516</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1513 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Passing input embeddings is currently not supported for {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class_</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1514 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1515 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1516 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1517 │   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1518 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519 │   │   │   </span>decoder_input_ids=decoder_input_ids,                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nsformers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1251</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1248 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1249 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1250 │   │   # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_att</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1251 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>decoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1252 │   │   │   </span>input_ids=decoder_input_ids,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1253 │   │   │   </span>attention_mask=decoder_attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1254 │   │   │   </span>encoder_hidden_states=encoder_outputs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>],                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nsformers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1107</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1106 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1107 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>layer_outputs = decoder_layer(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1108 │   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1109 │   │   │   │   │   </span>attention_mask=attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1110 │   │   │   │   │   </span>encoder_hidden_states=encoder_hidden_states,                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nsformers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">439</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 436 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 437 │   │   │   # cross_attn cached key/values tuple is at positions 3,4 of present_key_valu</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 438 │   │   │   </span>cross_attn_past_key_value = past_key_value[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> past_key_value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">Non</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 439 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>hidden_states, cross_attn_weights, cross_attn_present_key_value = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encod  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 440 │   │   │   │   </span>hidden_states=hidden_states,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 441 │   │   │   │   </span>key_value_states=encoder_hidden_states,                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 442 │   │   │   │   </span>attention_mask=encoder_attention_mask,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tor</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tra</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nsformers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">243</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 240 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 241 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Attention mask should be of size {</span>(bsz, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, tgt_len, src_len)<span style=\"color: #808000; text-decoration-color: #808000\">}, but</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 242 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 243 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attn_weights = attn_weights.view(bsz, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads, tgt_len, src_len) + at  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 244 │   │   │   </span>attn_weights = attn_weights.view(bsz * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads, tgt_len, src_len)      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 245 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 246 │   │   </span>attn_weights = nn.functional.softmax(attn_weights, dim=-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.93</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.26</span> GiB already \nallocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.06</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.41</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory try \nsetting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline(num_epochs=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# models = setup_models()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "1000000000000000019884624838656"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ainize/bart-base-cnn\")\n",
    "tokenizer.model_max_length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "assert id(models[\"summarization\"].model.shared) == id(models[\"classification\"].model.shared)\n",
    "assert id(models[\"summarization\"].model.encoder) == id(models[\"classification\"].model.encoder)\n",
    "assert id(models[\"summarization\"].model.decoder) == id(models[\"classification\"].model.decoder)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/tmp/ipykernel_56231/\u001B[0m\u001B[1;33m1601795630.py\u001B[0m:\u001B[94m1\u001B[0m in \u001B[92m<cell line: 1>\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_56231/1601795630.py'\u001B[0m                        \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mNameError: \u001B[0mname \u001B[32m'models'\u001B[0m is not defined\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_56231/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1601795630.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_56231/1601795630.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'models'</span> is not defined\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print_first_param(models[\"classification\"].model.encoder)\n",
    "print_first_param(models[\"summarization\"].model.encoder)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# we need a:\n",
    "#   -> dataframe loaded with docee examples\n",
    "#   -> tokenizer (bart tokenizer)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cls_train, cls_eval, summ_train, summ_eval = setup_dummy_dataset(\n",
    "    cls_train_size=10, cls_eval_size=4, summ_train_size=55, summ_eval_size=17\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_templates = {\n",
    "    \"classification\": {\n",
    "        \"train_dataset\": cls_train,\n",
    "        \"eval_dataset\": cls_eval,\n",
    "        \"batch_size\": 1,\n",
    "        \"collate_fn\": DataCollatorWithPadding(\n",
    "            tokenizer=tokenizer,\n",
    "            padding=PaddingStrategy.MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"train_dataset\": summ_train,\n",
    "        \"eval_dataset\": summ_eval,\n",
    "        \"batch_size\": 1,\n",
    "        \"collate_fn\": DataCollatorForSeq2Seq(\n",
    "            tokenizer=tokenizer,\n",
    "            padding=PaddingStrategy.MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    },\n",
    "}\n",
    "\n",
    "tasks = setup_tasks(task_templates, num_epochs=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(tasks, num_epochs=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# bruhus\n",
    "summ_dataset = load_dataset(\"cnn_dailymail\", name=\"3.0.0\")\n",
    "print({split: len(summ_dataset[split]) for split in summ_dataset})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# okay, we got this\n",
    "# cls_dataset = load_dataset(\"csv\", data_files=\"../data/docee/train_all.csv\")\n",
    "# data_files can be a dictionary, where key is the name of the split, and value is path to the split\n",
    "cls_dataset = load_dataset(\"csv\", data_files={\n",
    "    \"train\": \"../data/docee/18091999/train.csv\",\n",
    "    \"validation\": \"../data/docee/18091999/early_stopping.csv\"\n",
    "})\n",
    "cls_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cls_dataset[\"train\"].shuffle(42).select(range(100))[:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 100\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_cnn = summ_dataset.map(process_summary_example, batched=True, remove_columns=[\"id\", \"article\", \"highlights\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_cnn[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#import evaluate\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "\n",
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\"\n",
    "\n",
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary],\n",
    "    references=[reference_summary]\n",
    ")\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "summ_data_collator = DataCollatorForSeq2Seq(tokenizer, model=models[\"summarization\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = [tokenized_cnn[i] for i in range(2)]\n",
    "features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summ_data_collator(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_cnn.set_format(\"torch\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_cnn,\n",
    "    shuffle=True,\n",
    "    collate_fn=summ_data_collator,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_cnn,\n",
    "    collate_fn=summ_data_collator,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(models[\"summarization\"].parameters(), lr=2e-5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    models[\"summarization\"], optimizer, train_dataloader, eval_dataloader\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_train_epochs), total=num_train_epochs, desc=\"Epoch progress\"):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Epoch step\", leave=False):\n",
    "        # pass through model\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # okay, this works\n",
    "        # but we cannot iterate over the two dataloaders with knowing which batch we got\n",
    "        #   which means -> we got to do the iteration manually\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            )  # aha! we can plug the generation parameters here\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            # Replace -100 in the labels as we can't decode them\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels\n",
    "            )\n",
    "\n",
    "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "            # evaluation loop is fine for summarization but we need it for classification as well\n",
    "\n",
    "    # Compute metrics\n",
    "    result = rouge_score.compute()\n",
    "    # Extract the median ROUGE scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    print(f\"Epoch {epoch}:\", result)\n",
    "\n",
    "    output_dir = \"./test_summ_train\"\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_first_param(models[\"classification\"].model.encoder)\n",
    "print_first_param(models[\"summarization\"].model.encoder)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "cls_param_list = get_param_list(models[\"classification\"].model)\n",
    "summ_param_list = get_param_list(models[\"summarization\"].model)\n",
    "for cls_param, summ_param in zip(cls_param_list, summ_param_list):\n",
    "    if not torch.all(torch.eq(cls_param, summ_param)):\n",
    "        raise RuntimeError(\"Shared parameters are not equal!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, param in models[\"classification\"].named_parameters():\n",
    "    print(f\"{name} is {param}\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "event_names = cls_dataset.unique(\"event_type\")\n",
    "event_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cls_dataset = cls_dataset.cast_column(\"event_type\", ClassLabel(num_classes=len(event_names[\"train\"]), names=sorted(event_names[\"train\"])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cls_dataset[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cls_dataset[\"train\"][0][\"text\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "docee = cls_dataset.map(preprocess_docee, batched=True, remove_columns=cls_dataset[\"train\"].column_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docee[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 1\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer,\n",
    "    padding=PaddingStrategy.LONGEST,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    docee[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    docee[\"validation\"],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "{k: v.shape for k, v in batch.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test run\n",
    "outputs = models[\"classification\"](**batch)\n",
    "print(f\"{outputs.loss = }\")\n",
    "print(f\"{outputs.logits.shape = }\")\n",
    "# moze"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cls_optimizer = AdamW(model.parameters(), lr=5e-5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=cls_optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "num_training_steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cls_accelerator = Accelerator()\n",
    "train_dataloader, eval_dataloader, model, optimizer = cls_accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, models[\"classification\"], cls_optimizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in tqdm(eval_dataloader, total=len(eval_dataloader), desc=\"Evaluation\"):\n",
    "    # extract outputs\n",
    "    outputs = model(**batch)\n",
    "    # print(outputs.keys())  # loss, logits, encoder_last_hidden_state\n",
    "\n",
    "    # outputs[\"logits\"] = (BS, 59)\n",
    "    # we need argmax by dimension 1\n",
    "\n",
    "    # decode logits into labels\n",
    "    predictions = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "    # print(labels)\n",
    "    f1.add_batch(\n",
    "        predictions=predictions.cpu().numpy(),\n",
    "        references=batch[\"labels\"].cpu().numpy(),\n",
    "    )\n",
    "    # break\n",
    "    # f1.add_batch(predictions=outputs[\"labels\"])\n",
    "result = f1.compute(average=\"macro\")\n",
    "print(result)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_micro = f1.compute(average=\"micro\")\n",
    "print(f1_micro)\n",
    "# okay, so we cannot call compute multiple times"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in tqdm(range(num_epochs), total=num_epochs, desc=\"Epoch progress\"):\n",
    "    for batch in tqdm(train_dataloader, total=len(train_dataloader), desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        cls_accelerator.backward(loss)\n",
    "\n",
    "        cls_optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        cls_optimizer.zero_grad()\n",
    "\n",
    "    # eval loop\n",
    "    # model.eval()\n",
    "    # we need metrics\n",
    "    # for batch in tqdm(eval_dataloader, total=len(eval_dataloader), desc=f\"Evaluation after epoch {epoch+1}\", leave=False):\n",
    "    # what about the evaluation loop? -> stick it somewhere here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_first_param(models[\"classification\"].model.encoder)\n",
    "print_first_param(models[\"summarization\"].model.encoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_epochs = 1\n",
    "cls_steps = 1  # what does this mean?\n",
    "summ_steps = 2   # what does this mean?\n",
    "\n",
    "cls_batch_size=1\n",
    "summ_batch_size=1\n",
    "\n",
    "# probably doesnt make much sense to train summarization more often than classification, right?\n",
    "# the thing is:\n",
    "#   we are actually learning SUMMARIZATION!!\n",
    "#   -> but we want to accomplish learning this summarization by utilizing classification as well\n",
    "\n",
    "cls_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=PaddingStrategy.MAX_LENGTH,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "summ_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=PaddingStrategy.MAX_LENGTH,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tasks = {\n",
    "    \"classification\": {\n",
    "        \"model\": models[\"classification\"],\n",
    "        \"optimizer\": None,\n",
    "        \"train_dataloader\": DataLoader(\n",
    "            docee[\"train\"],\n",
    "            batch_size=cls_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=cls_collator\n",
    "        ),\n",
    "        \"eval_dataloader\": DataLoader(\n",
    "            docee[\"validation\"],\n",
    "            batch_size=cls_batch_size,\n",
    "            collate_fn=cls_collator\n",
    "        )\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"model\": models[\"summarization\"],\n",
    "        \"optimizer\": None,\n",
    "        \"train_dataloader\": DataLoader(\n",
    "            tokenized_cnn[\"train\"],\n",
    "            batch_size=summ_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=summ_collator\n",
    "        ),\n",
    "        \"eval_dataloader\": DataLoader(\n",
    "            tokenized_cnn[\"validation\"],\n",
    "            batch_size=summ_batch_size,\n",
    "            collate_fn=summ_collator\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "setup_optimizers(tasks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the thing is, classification dataloader contains less examples than summarization dataloader\n",
    "# we can solve this by oversampling the classification dataloader (by using itertools.tee)\n",
    "summ_cls_ratio = len(tasks[\"summarization\"][\"train_dataloader\"]) // len(tasks[\"classification\"][\"train_dataloader\"]) + 1\n",
    "summ_cls_ratio\n",
    "# tasks[\"classification\"][\"train_dataloader\"] = tee(tasks)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tasks[\"classification\"][\"train_dataloader\"] = DataLoader(\n",
    "    docee[\"train\"],\n",
    "    batch_size=cls_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=cls_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# set_train(tasks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accelerate(tasks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tasks[\"classification\"][\"train_dataloader\"] = chain(*tee(tasks[\"classification\"][\"train_dataloader\"], summ_cls_ratio))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_iter = iter(tasks[\"classification\"][\"train_dataloader\"])\n",
    "batch = next(test_iter)\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "fix_cls_dataloader(tasks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "setup_schedulers(tasks)\n",
    "tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_train(tasks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# pa ovo radi buraz\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# okay, lets try this, but with an extremely small size, just to test the loop\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cls_train, cls_eval, summ_train, summ_eval = setup_dummy_dataset(\n",
    "    cls_train_size=10,\n",
    "    cls_eval_size=5,\n",
    "    summ_train_size=55,\n",
    "    summ_eval_size=17\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
