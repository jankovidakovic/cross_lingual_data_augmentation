{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   id                                              title  \\\n0   0  Vietnam reelects conservative Nguyễn Phú Trọng...   \n1   1  At least 42 people are killed in a bus crash i...   \n2   2  At least 27 migrants die in a shipwreck in the...   \n3   3  Colten Treu faces charges of vehicular homicid...   \n4   4  Hours after the announcement, Morales resigns ...   \n\n                                                text  \\\n0  Vietnam's Communist Party Wednesday re-elected...   \n1  Another 43 people were injured when the bus ca...   \n2  At least 27 migrants have died off the Turkish...   \n3  Colten Treu, 21, and his roommate both told au...   \n4  Bolivian President Evo Morales has resigned af...   \n\n                                      event_type  \\\n0               Government Job change - Election   \n1                                     Road Crash   \n2                                      Shipwreck   \n3                                     Road Crash   \n4  Government Job change - Resignation_Dismissal   \n\n                                           arguments           date  \\\n0  [{'start': 0, 'end': 24, 'type': 'Candidates a...   January 2016   \n1  [{'start': 8, 'end': 29, 'type': 'Casualties a...   October 2006   \n2  [{'start': 0, 'end': 29, 'type': 'Casualties a...  February 2016   \n3  [{'start': 183, 'end': 207, 'type': 'Number of...  November 2018   \n4  [{'start': 0, 'end': 17, 'type': 'Position', '...  November 2019   \n\n                                            metadata  \n0        ['(AP via ABC News)', '(Channel NewsAsia)']  \n1                                          ['(BBC)']  \n2  ['(ANSAmed)', '(Leadership)', '(news.com.au)',...  \n3                             ['(KSTP)', '(Oxygen)']  \n4                   ['(BBC News)', '(The Guardian)']  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>text</th>\n      <th>event_type</th>\n      <th>arguments</th>\n      <th>date</th>\n      <th>metadata</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Vietnam reelects conservative Nguyễn Phú Trọng...</td>\n      <td>Vietnam's Communist Party Wednesday re-elected...</td>\n      <td>Government Job change - Election</td>\n      <td>[{'start': 0, 'end': 24, 'type': 'Candidates a...</td>\n      <td>January 2016</td>\n      <td>['(AP via ABC News)', '(Channel NewsAsia)']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>At least 42 people are killed in a bus crash i...</td>\n      <td>Another 43 people were injured when the bus ca...</td>\n      <td>Road Crash</td>\n      <td>[{'start': 8, 'end': 29, 'type': 'Casualties a...</td>\n      <td>October 2006</td>\n      <td>['(BBC)']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>At least 27 migrants die in a shipwreck in the...</td>\n      <td>At least 27 migrants have died off the Turkish...</td>\n      <td>Shipwreck</td>\n      <td>[{'start': 0, 'end': 29, 'type': 'Casualties a...</td>\n      <td>February 2016</td>\n      <td>['(ANSAmed)', '(Leadership)', '(news.com.au)',...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Colten Treu faces charges of vehicular homicid...</td>\n      <td>Colten Treu, 21, and his roommate both told au...</td>\n      <td>Road Crash</td>\n      <td>[{'start': 183, 'end': 207, 'type': 'Number of...</td>\n      <td>November 2018</td>\n      <td>['(KSTP)', '(Oxygen)']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Hours after the announcement, Morales resigns ...</td>\n      <td>Bolivian President Evo Morales has resigned af...</td>\n      <td>Government Job change - Resignation_Dismissal</td>\n      <td>[{'start': 0, 'end': 17, 'type': 'Position', '...</td>\n      <td>November 2019</td>\n      <td>['(BBC News)', '(The Guardian)']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/docee/train_all.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "21949"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "text    21864\ndtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, [\"text\"]].nunique()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "21779"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# well shit\n",
    "# okay, lets remove those\n",
    "\n",
    "# they might possibly be the source of motherfucking data leakage\n",
    "deduplicated_df = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "len(deduplicated_df)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['id', 'title', 'text', 'event_type', 'arguments', 'date', 'metadata'], dtype='object')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11130/2805648092.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deduplicated_df.drop(columns=[\"id\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "deduplicated_df.drop(columns=[\"id\"], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "RangeIndex(start=0, stop=21779, step=1)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_df.reset_index(drop=True, inplace=True)\n",
    "deduplicated_df.index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['title', 'text', 'event_type', 'arguments', 'date', 'metadata'], dtype='object')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "deduplicated_df.to_csv(\"../data/docee/train_all.csv\", index_label=\"id\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "\n",
    "def deduplicate(\n",
    "        dataset_path: str,\n",
    "        subset: Optional[list[str]]=None,\n",
    "        old_index_name: str=\"id\",\n",
    "        new_index_name: str=\"id\",\n",
    "        write_out=print\n",
    "):\n",
    "    if subset is None:\n",
    "        subset = [\"text\"]\n",
    "\n",
    "    # load df\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    write_out(f\"Loaded {len(df)} rows from {os.path.abspath(dataset_path)}\")\n",
    "\n",
    "    # find duplicates\n",
    "    duplicates: pd.DataFrame = df.loc[df.duplicated(subset=subset, keep=False), :]\n",
    "    write_out(f\"With respect to columns {pformat(subset)}, {len(duplicates)} duplicate examples were found.\")\n",
    "\n",
    "    # group duplicates by label\n",
    "    # because we only want to drop duplicates which actually have different labels\n",
    "    group_labels = duplicates.groupby(by=subset).event_type\n",
    "    write_out(f\"Length \")\n",
    "\n",
    "    # znas sta, doslovno je nebitno sad\n",
    "\n",
    "    # just remove all the fucking duplicates, not that hard\n",
    "\n",
    "    pd.read_csv(dataset_path)\\\n",
    "        .drop_duplicates(subset=subset, keep=False)\\\n",
    "        .drop(columns=[old_index_name])\\\n",
    "        .reset_index(drop=True)\\\n",
    "        .to_csv(dataset_path, index_label=new_index_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "deduplicate(\"../data/docee/train_all.csv\", old_index_name=\"id\", new_index_name=\"index\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "deduplicate(\"../data/docee/test_all.csv\", old_index_name=\"index\", new_index_name=\"index\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pd.read_csv(train_path)) = 21779\n",
      "len(pd.read_csv(test_path)) = 5526\n"
     ]
    }
   ],
   "source": [
    "train_path = \"../data/docee/train_all.csv\"\n",
    "test_path = \"../data/docee/test_all.csv\"\n",
    "\n",
    "print(f\"{len(pd.read_csv(train_path)) = }\")\n",
    "print(f\"{len(pd.read_csv(test_path)) = }\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "event_type    59\ndtype: int64"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, [\"event_type\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                              title  \\\n0           0  The anti-Syrian government and the pro-Syrian ...   \n1           1  Israel’s PM Benjamin Netanyahu reportedly dela...   \n2           2  A shooting at a billiard hall in El Tarra, Col...   \n3           3  Under new guidelines to come into force from A...   \n4           4  Newark, New Jersey's 30 public schools shut of...   \n\n                                                text  \\\n0  Rival Lebanese leaders have agreed on steps to...   \n1  The postponement of the military ground offens...   \n2  Nine people have been killed in a gun attack i...   \n3  Private clinics that charge for pregnancy serv...   \n4  NEW YORK (Reuters) - Water fountains at 30 sch...   \n\n                  event_type  \\\n0             Sign Agreement   \n1             Armed Conflict   \n2                       Riot   \n3  Government Policy Changes   \n4      Environment Pollution   \n\n                                           arguments           date  \\\n0  [{'start': 148, 'end': 206, 'type': 'Contracti...       May 2008   \n1  [{'start': 725, 'end': 736, 'type': 'Target', ...  November 2012   \n2  [{'start': 0, 'end': 27, 'type': 'Casualties a...      July 2018   \n3  [{'start': 0, 'end': 135, 'type': 'Policy Cont...   January 2012   \n4  [{'start': 54, 'end': 71, 'type': 'Location', ...     March 2016   \n\n                                            metadata  \n0  ['(giving it a veto)', '(BBC News)', '(Bloombe...  \n1                                  ['(RT)', '(BBC)']  \n2                                          ['(BBC)']  \n3                                          ['(BBC)']  \n4                                      ['(Reuters)']  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>text</th>\n      <th>event_type</th>\n      <th>arguments</th>\n      <th>date</th>\n      <th>metadata</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>The anti-Syrian government and the pro-Syrian ...</td>\n      <td>Rival Lebanese leaders have agreed on steps to...</td>\n      <td>Sign Agreement</td>\n      <td>[{'start': 148, 'end': 206, 'type': 'Contracti...</td>\n      <td>May 2008</td>\n      <td>['(giving it a veto)', '(BBC News)', '(Bloombe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Israel’s PM Benjamin Netanyahu reportedly dela...</td>\n      <td>The postponement of the military ground offens...</td>\n      <td>Armed Conflict</td>\n      <td>[{'start': 725, 'end': 736, 'type': 'Target', ...</td>\n      <td>November 2012</td>\n      <td>['(RT)', '(BBC)']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>A shooting at a billiard hall in El Tarra, Col...</td>\n      <td>Nine people have been killed in a gun attack i...</td>\n      <td>Riot</td>\n      <td>[{'start': 0, 'end': 27, 'type': 'Casualties a...</td>\n      <td>July 2018</td>\n      <td>['(BBC)']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Under new guidelines to come into force from A...</td>\n      <td>Private clinics that charge for pregnancy serv...</td>\n      <td>Government Policy Changes</td>\n      <td>[{'start': 0, 'end': 135, 'type': 'Policy Cont...</td>\n      <td>January 2012</td>\n      <td>['(BBC)']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Newark, New Jersey's 30 public schools shut of...</td>\n      <td>NEW YORK (Reuters) - Water fountains at 30 sch...</td>\n      <td>Environment Pollution</td>\n      <td>[{'start': 54, 'end': 71, 'type': 'Location', ...</td>\n      <td>March 2016</td>\n      <td>['(Reuters)']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyze length of text\n",
    "train_df = df\n",
    "test_df = pd.read_csv(\"../data/docee/test_all.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE=18091999\n",
    "\n",
    "dev_df, test_df = train_test_split(test_df, test_size=0.5, random_state=RANDOM_STATE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def count_unique_labels(df):\n",
    "    return df.loc[:, [\"event_type\"]].nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_type    59\n",
      "dtype: int64\n",
      "event_type    59\n",
      "dtype: int64\n",
      "event_type    59\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_labels(train_df))\n",
    "print(count_unique_labels(dev_df))\n",
    "print(count_unique_labels(test_df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class DoceeDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        super().__init__()\n",
    "        self.text = df.text\n",
    "        self.labels = df.event_type\n",
    "        self.length = len(self.text)\n",
    "\n",
    "    def len(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.text[idx], self.labels[idx]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "train_dataset = DoceeDataset(train_df)\n",
    "dev_dataset = DoceeDataset(dev_df)\n",
    "test_dataset = DoceeDataset(test_df)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text = 'Vietnam\\'s Communist Party Wednesday re-elected its 71-year-old chief for a second term, an expected outcome that sees the conservative pro-China ideologue cementing his hold on power.\\nThe party\\'s congress elected Nguyen Phu Trong (pronounced Noo-yen Foo Chong) to a 19-member Politburo, the all-powerful body that handles the day-to-day affairs of the government and the party. In a subsequent vote, he was immediately chosen as the general-secretary, the de facto No. 1 leader of the country.\\nThe announcement was made on the official Vietnam News Agency\\'s website.\\nOfficials said Deputy Prime Minister Nguyen Xuan Phuc was also elected to the Politburo, and he is now expected to become the prime minister. He will replace Nguyen Tan Dung, who had had led economic reforms over the last 10 years and had harbored ambitions for the top job. His challenge, however, was snuffed by Trong\\'s supporters during the weeklong party congress that ends Thursday.\\nThe third most important member elected to the Politburo was Minister of Public Security Tran Dai Quang, who will be the country\\'s new president, said the officials, who spoke on condition of anonymity because they were not authorized to speak to the media.\\nThe general secretary, the prime minister and the president, along with the chairman of the National Assembly, are the four key members in the collective leadership represented by the Politburo, and the 180-member Central Committee, which handles policy.\\nThe renewal of the leadership means little change for Vietnam.\\nTrong is expected to continue to push Dung\\'s economic reforms. Despite having a reputation for being pro-China he is not likely to be totally subservient to Beijing as that would risk massive anger from ordinary Vietnamese who harbor a deep dislike and historical suspicion of China.\\n\"Many people were afraid that a conservative trend would prevail if Mr. Trong is re-elected. But ... whoever they may be, and however conservative they may be, when they are at the helm they are under pressure to carry out reforms,\" Le Hong Hiep, a visiting Vietnamese fellow at the Institute of Southeast Asia Studies in Singapore, told The Associated Press.\\nThe Communist Party is entitled by the constitution to govern and Vietnam\\'s 93 million people have no direct role in electing the leaders of the 4.5 million-member party.\\nIt is believed that as a compromise with Prime Minister Dung\\'s camp, Trong will not serve his full five-year term but may hand over power to another leader mid-way.\\nDung was seen as a pro-business leader who investors believe would have continued with economic reforms he set in motion 10 years ago that helped Vietnam attract a flood of foreign investment and was partly responsible for tripling the per capita GDP to $2,100. He was also seen as standing up to China, which is making aggressive territorial claims in the South China Sea and building islands, much to the chagrin of Southeast Asia nations who have conflicting claims in the waters.\\nChina sent an oil rig into Vietnamese waters in 2014, triggering a massive backlash among Vietnamese, including attacks on Chinese businesses. Dung was vocal in criticizing China then, while Trong was muted.\\nDespite Trong\\'s reputation as being an anti-thesis of Dung, the reality is not so black-and-white. Observers agree that the economic reforms Dung started have the blessings of the collective leadership, including Trong.\\nA clear example came when a plenum of the outgoing Central Committee overwhelmingly endorsed Vietnam joining the Trans-Pacific Partnership, a U.S. led free-trade initiative.\\nAs for China, Trong will likely not risk the ire of the public by being soft if Beijing\\'s assertiveness impinges on Vietnam territorial integrity.\\nThe cosmetic change in the leadership also means that Vietnam has no immediate hopes for political reforms, even though there is a desire in the government to loosen up on public freedoms.\\n\"They are faced with a dilemma. They want to maintain the one-party rule and at the same time they want to have reforms in some limited areas,\" said Hiep, the Vietnamese scholar.\\n\"Their trend is to change, but they will still be cautious, because the party\\'s ultimate goal is to maintain their monopoly on power,\" Hiep said.', label = 'Government Job change - Election'\n"
     ]
    }
   ],
   "source": [
    "text, label = train_dataset[0]\n",
    "print(f\"{text = }, {label = }\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=59, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "DEVICE=\"cuda:0\"\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    cache_dir=\"../pretrained_models\",\n",
    "    num_labels=59\n",
    ").to(DEVICE)  # TODO - figure out how was this initialized\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65b24afe52e640969168dc7ac1906794"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df56499e326a420188e482170f930514"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d9e7392049948349c56b0d32ffdc767"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoding = {'input_ids': tensor([[    0,   846,  5810,  8697,    18, 12416,  1643,   307,   769,    12,\n",
      "         15672,    63,  6121,    12,   180,    12,   279,   834,    13,    10,\n",
      "           200,  1385,     6,    41,   421,  4258,    14,  3681,     5,  3354,\n",
      "          1759,    12,  8481, 23432, 28789, 10546,   154,    39,   946,    15,\n",
      "           476,     4, 50118,   133,   537,    18, 12442,  2736, 19211,  4129,\n",
      "           257,  2393,  1657,    36, 26404, 20155,   440,   139,    12,   219,\n",
      "           225, 36155, 25932,    43,     7,    10,   753,    12,  8648, 16373,\n",
      "           428,  7367,     6,     5,    70,    12, 35866,   809,    14, 14617,\n",
      "             5,   183,    12,   560,    12,  1208,  5185,     9,     5,   168,\n",
      "             8,     5,   537,     4,    96,    10,  7757,   900,     6,    37,\n",
      "            21,  1320,  4986,    25,     5,   937,    12, 19301,  1766,     6,\n",
      "             5,   263, 20624,   440,     4,   112,   884,     9,     5,   247,\n",
      "             4, 50118,   133,  2443,    21,   156,    15,     5,   781,  5490,\n",
      "           491,  3131,    18,   998,     4, 50118, 35779,    26,  4269,  1489,\n",
      "           692, 19211, 44730,  4129,  3964,    21,    67,  2736,     7,     5,\n",
      "         16373,   428,  7367,     6,     8,    37,    16,   122,   421,     7,\n",
      "           555,     5,  2654,  1269,     4,    91,    40,  3190, 19211,  7650,\n",
      "           211,  1545,     6,    54,    56,    56,   669,   776,  4907,    81,\n",
      "             5,    94,   158,   107,     8,    56, 32435,  3995, 12294,    13,\n",
      "             5,   299,   633,     4,   832,  1539,     6,   959,     6,    21,\n",
      "          4543, 16368,    30,  2393,  1657,    18,  2732,   148,     5,   186,\n",
      "          3479,   537, 12442,    14,  3587,   296,     4, 50118,   133,   371,\n",
      "           144,   505,   919,  2736,     7,     5, 16373,   428,  7367,    21,\n",
      "           692,     9,  1909,  2010,  2393,   260, 26270,  3232,  1097,     6,\n",
      "            54,    40,    28,     5,   247,    18,    92,   394,     6,    26,\n",
      "             5,   503,     6,    54,  1834,    15,  1881,     9,  7901,   142,\n",
      "            51,    58,    45,  8672,     7,  1994,     7,     5,   433,     4,\n",
      "         50118,   133,   937,  2971,     6,     5,  2654,  1269,     8,     5,\n",
      "           394,     6,   552,    19,     5,  2243,     9,     5,   496,  3389,\n",
      "             6,    32,     5,   237,   762,   453,    11,     5,  6981,  1673,\n",
      "          4625,    30,     5, 16373,   428,  7367,     6,     8,     5,  8963,\n",
      "            12,  8648,  1505,  1674,     6,    61, 14617,   714,     4, 50118,\n",
      "           133, 14573,     9,     5,  1673,   839,   410,   464,    13,  5490,\n",
      "             4, 50118, 12667,  1657,    16,   421,     7,   535,     7,  1920,\n",
      "           211,  1545,    18,   776,  4907,     4,  2285,   519,    10,  5070,\n",
      "            13,   145,  1759,    12,  8481,    37,    16,    45,   533,     7,\n",
      "            28,  4940,  2849, 31499,  4843,     7,  3332,    25,    14,    74,\n",
      "           810,  2232,  6378,    31,  7945, 16859,    54, 19655,    10,  1844,\n",
      "         28101,     8,  4566,  8551,     9,   436,     4, 50118,   113, 10787,\n",
      "            82,    58,  6023,    14,    10,  3354,  2904,    74, 19817,   114,\n",
      "           427,     4,  2393,  1657,    16,   769,    12, 15672,     4,   125,\n",
      "          1666, 14051,    51,   189,    28,     6,     8,   959,  3354,    51,\n",
      "           189,    28,     6,    77,    51,    32,    23,     5, 13885,    51,\n",
      "            32,   223,  1164,     7,  2324,    66,  4907,    60,  1063,  3523,\n",
      "           289,   324,   642,     6,    10,  3918, 16859,  2598,    23,     5,\n",
      "          2534,     9,  6079,  1817,  9307,    11,  2920,     6,   174,    20,\n",
      "          1562,   977,     4, 50118,   133, 12416,  1643,    16,  7919,    30,\n",
      "             5,  7255,     7, 11738,     8,  5490,    18,  8060,   153,    82,\n",
      "            33,   117,  2228,   774,    11, 31839,     5,   917,     9,     5,\n",
      "           204,     4,   245,   153,    12,  8648,   537,     4, 50118,   243,\n",
      "            16,  2047,    14,    25,    10,  7932,    19,  1489,   692,   211,\n",
      "          1545,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", cache_dir=\"../pretrained_models\")\n",
    "\n",
    "batch_encoding = tokenizer(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")  # so why do we have the option to pad here\n",
    "print(f\"{batch_encoding = }\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_output = SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0779,  0.0104,  0.1149, -0.1154,  0.1039,  0.1422,  0.0217,  0.1132,\n",
      "          0.0399, -0.0130, -0.1646, -0.1809,  0.1099,  0.1394, -0.1584,  0.1199,\n",
      "          0.0621,  0.0304,  0.0869, -0.0747,  0.0268,  0.0345,  0.1449,  0.0141,\n",
      "         -0.2147,  0.1238,  0.1111, -0.0012, -0.0752, -0.0665,  0.0020,  0.0898,\n",
      "          0.0056,  0.0688,  0.2843,  0.2413,  0.1161,  0.0048, -0.1190, -0.1027,\n",
      "         -0.0071, -0.0091,  0.1275,  0.0508,  0.1295, -0.0038, -0.0877, -0.1466,\n",
      "          0.1489, -0.1046, -0.0979, -0.0397, -0.0842,  0.0493,  0.0921,  0.0401,\n",
      "         -0.0289,  0.0136,  0.1131]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# input_ids, attention_mask\n",
    "batch_encoding.to(DEVICE)\n",
    "model_output = model(**batch_encoding)\n",
    "print(f\"{model_output = }\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text']",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 16>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# okay, forward pass works\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# next, we need a data collator\u001B[39;00m\n\u001B[1;32m      9\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorWithPadding(\n\u001B[1;32m     10\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[1;32m     11\u001B[0m     padding\u001B[38;5;241m=\u001B[39mPaddingStrategy\u001B[38;5;241m.\u001B[39mLONGEST,\n\u001B[1;32m     12\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mmodel_max_length,\n\u001B[1;32m     13\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     14\u001B[0m )\n\u001B[0;32m---> 16\u001B[0m data_collator_output \u001B[38;5;241m=\u001B[39m \u001B[43mdata_collator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_collator_output \u001B[38;5;132;01m= }\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transformers/data/data_collator.py:247\u001B[0m, in \u001B[0;36mDataCollatorWithPadding.__call__\u001B[0;34m(self, features)\u001B[0m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: List[Dict[\u001B[38;5;28mstr\u001B[39m, Any]]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m--> 247\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[1;32m    255\u001B[0m         batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2900\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.pad\u001B[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001B[0m\n\u001B[1;32m   2898\u001B[0m \u001B[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001B[39;00m\n\u001B[1;32m   2899\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_input_names[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m encoded_inputs:\n\u001B[0;32m-> 2900\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2901\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou should supply an encoding or a list of encodings to this method \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2902\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthat includes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_input_names[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but you provided \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(encoded_inputs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2903\u001B[0m     )\n\u001B[1;32m   2905\u001B[0m required_input \u001B[38;5;241m=\u001B[39m encoded_inputs[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_input_names[\u001B[38;5;241m0\u001B[39m]]\n\u001B[1;32m   2907\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m required_input:\n",
      "\u001B[0;31mValueError\u001B[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text']"
     ]
    }
   ],
   "source": [
    "from transformers.utils import PaddingStrategy\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# okay, forward pass works\n",
    "\n",
    "# next, we need a data collator\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(  # this is obviously not something that we need\n",
    "    tokenizer=tokenizer,\n",
    "    padding=PaddingStrategy.LONGEST,\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "data_collator_output = data_collator({\"text\":text})\n",
    "print(f\"{data_collator_output = }\")\n",
    "\n",
    "# sampler -> getitem -> tokenizer -> move_to_device\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cross_lingual_data_augmentation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8c49b0963e22f9647a08a6fe4c9c3b2e58d71051e16d9f93d5868752273fb3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
