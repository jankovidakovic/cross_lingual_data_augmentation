{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from transformers.utils import PaddingStrategy\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "tokenizer_path = \"/shared/lovorka/jvidakovic/models/checkpoint-28000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    padding=PaddingStrategy.LONGEST,\n",
    "    use_fast=True,\n",
    "    model_max_length=1024\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "1024"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "model_path = os.path.join(tokenizer_path, \"summarization\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "BartForConditionalGeneration(\n  (model): BartModel(\n    (shared): Embedding(50265, 768, padding_idx=1)\n    (encoder): BartEncoder(\n      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    ")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.bart.configuration_bart.BartConfig"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cpu\",\n",
    "    framework=\"pt\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                              title  \\\n0           0  Vietnam reelects conservative Nguyễn Phú Trọng...   \n1           1  At least 42 people are killed in a bus crash i...   \n2           2  At least 27 migrants die in a shipwreck in the...   \n3           3  Colten Treu faces charges of vehicular homicid...   \n4           4  Hours after the announcement, Morales resigns ...   \n\n                                                text  \\\n0  Vietnam's Communist Party Wednesday re-elected...   \n1  Another 43 people were injured when the bus ca...   \n2  At least 27 migrants have died off the Turkish...   \n3  Colten Treu, 21, and his roommate both told au...   \n4  Bolivian President Evo Morales has resigned af...   \n\n                                      event_type  \\\n0               Government Job change - Election   \n1                                     Road Crash   \n2                                      Shipwreck   \n3                                     Road Crash   \n4  Government Job change - Resignation_Dismissal   \n\n                                           arguments           date  \\\n0  [{'start': 0, 'end': 24, 'type': 'Candidates a...   January 2016   \n1  [{'start': 8, 'end': 29, 'type': 'Casualties a...   October 2006   \n2  [{'start': 0, 'end': 29, 'type': 'Casualties a...  February 2016   \n3  [{'start': 183, 'end': 207, 'type': 'Number of...  November 2018   \n4  [{'start': 0, 'end': 17, 'type': 'Position', '...  November 2019   \n\n                                            metadata  \n0        ['(AP via ABC News)', '(Channel NewsAsia)']  \n1                                          ['(BBC)']  \n2  ['(ANSAmed)', '(Leadership)', '(news.com.au)',...  \n3                             ['(KSTP)', '(Oxygen)']  \n4                   ['(BBC News)', '(The Guardian)']  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>text</th>\n      <th>event_type</th>\n      <th>arguments</th>\n      <th>date</th>\n      <th>metadata</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Vietnam reelects conservative Nguyễn Phú Trọng...</td>\n      <td>Vietnam's Communist Party Wednesday re-elected...</td>\n      <td>Government Job change - Election</td>\n      <td>[{'start': 0, 'end': 24, 'type': 'Candidates a...</td>\n      <td>January 2016</td>\n      <td>['(AP via ABC News)', '(Channel NewsAsia)']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>At least 42 people are killed in a bus crash i...</td>\n      <td>Another 43 people were injured when the bus ca...</td>\n      <td>Road Crash</td>\n      <td>[{'start': 8, 'end': 29, 'type': 'Casualties a...</td>\n      <td>October 2006</td>\n      <td>['(BBC)']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>At least 27 migrants die in a shipwreck in the...</td>\n      <td>At least 27 migrants have died off the Turkish...</td>\n      <td>Shipwreck</td>\n      <td>[{'start': 0, 'end': 29, 'type': 'Casualties a...</td>\n      <td>February 2016</td>\n      <td>['(ANSAmed)', '(Leadership)', '(news.com.au)',...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Colten Treu faces charges of vehicular homicid...</td>\n      <td>Colten Treu, 21, and his roommate both told au...</td>\n      <td>Road Crash</td>\n      <td>[{'start': 183, 'end': 207, 'type': 'Number of...</td>\n      <td>November 2018</td>\n      <td>['(KSTP)', '(Oxygen)']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Hours after the announcement, Morales resigns ...</td>\n      <td>Bolivian President Evo Morales has resigned af...</td>\n      <td>Government Job change - Resignation_Dismissal</td>\n      <td>[{'start': 0, 'end': 17, 'type': 'Position', '...</td>\n      <td>November 2019</td>\n      <td>['(BBC News)', '(The Guardian)']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_path = \"/home/jvidakovic/cross_lingual_data_augmentation/data/docee/all/train_all.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df = df.loc[:, [\"text\", \"title\", \"event_type\", \"date\"]]\n",
    "summary_df = df.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def get_class_counts(df: pd.DataFrame) -> dict[str, int]:\n",
    "    \"\"\"For each class label present in the dataset, returns the\n",
    "    count of examples for that class.\n",
    "\n",
    "    :param df:  dataframe\n",
    "    :return:  dictionary where each key represents a class label\n",
    "        and each value represents the count of examples belonging to\n",
    "        that class.\n",
    "    \"\"\"\n",
    "\n",
    "    class_names = set(df[\"event_type\"].tolist())\n",
    "    print(f\"Total of {len(class_names)} class names.\")\n",
    "\n",
    "    class_counts = {\n",
    "        class_name: np.sum(df.event_type.values == class_name)\n",
    "        for class_name in class_names\n",
    "    }\n",
    "    print(f\"Sum of all class counts equals {sum(class_counts.values())}.\")\n",
    "    return class_counts\n",
    "\n",
    "def low_resource_slice(\n",
    "        df: pd.DataFrame,\n",
    "        cutoff: int,\n",
    "        return_classes: bool = False\n",
    ") -> pd.DataFrame | Tuple[list[str], pd.DataFrame]:\n",
    "    \"\"\" For a given dataframe, returns all examples which belong to low resource classes.\n",
    "\n",
    "    Low resource classes include all classes for which the class count (i.e. number of examples)\n",
    "    is not greater than the given cutoff.\n",
    "\n",
    "    :param return_classes: whether or not to return classes\n",
    "    :param df:  dataframe\n",
    "    :param cutoff:  low resource threshold\n",
    "    :return:  (low_resource_classes, low_resource_df)\n",
    "    \"\"\"\n",
    "\n",
    "    class_counts = get_class_counts(df)\n",
    "    low_resource_classes = list(filter(lambda k: class_counts[k] <= cutoff, class_counts))\n",
    "    low_resource_df = df.loc[df[\"event_type\"].isin(low_resource_classes), :]\n",
    "    if return_classes:\n",
    "        return low_resource_classes, low_resource_df\n",
    "    else:\n",
    "        return low_resource_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 59 class names.\n",
      "Sum of all class counts equals 21949.\n",
      "low_resource_classes = ['Tsunamis', 'Mudslides', 'Organization Closed', 'Famous Person - Give a speech', 'Hurricanes_Tornado_Storm_Blizzard', 'Join in an Organization', 'Famous Person - Commit Crime - Investigate', 'Diplomatic Visit', 'Withdraw from an Organization', 'Famous Person - Commit Crime - Release', 'Famous Person - Commit Crime - Arrest', 'Floods', 'Famous Person - Recovered', 'Famine', 'Strike', 'Famous Person - Divorce', 'Sign Agreement', 'Famous Person - Marriage', 'Shipwreck', 'Mass Poisoning', 'Diplomatic Talks _ Diplomatic_Negotiation_ Summit Meeting', 'Organization Fine', 'Tear Up Agreement', 'Awards ceremony', 'Train collisions', 'Mine Collapses', 'Financial Aid', 'Financial Crisis', 'Road Crash', 'Environment Pollution', 'Famous Person - Death', 'Famous Person - Sick', 'Military Exercise', 'Volcano Eruption', 'New achievements in aerospace', 'Insect Disaster', 'New archeological discoveries', 'Disease Outbreaks', 'Regime Change', 'Bank Robbery', 'Gas explosion', 'Government Job change - Appoint_Inauguration', 'Droughts', 'Organization Merge', 'Organization Established', 'New wonders in nature', 'Break historical records']\n",
      "len(low_resource_classes) = 47\n"
     ]
    }
   ],
   "source": [
    "low_resource_classes, summary_df = low_resource_slice(\n",
    "    summary_df,\n",
    "    500,\n",
    "    return_classes=True\n",
    ")\n",
    "print(f\"{low_resource_classes = }\")\n",
    "print(f\"{len(low_resource_classes) = }\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([    1,     2,     3, ..., 21945, 21946, 21948])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_doc_ids = summary_df.index.values\n",
    "source_doc_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Optional, Callable, Iterable\n",
    "\n",
    "def concat_dot_join(tokens: Iterable[str]) -> str:\n",
    "    return \". \".join(tokens)\n",
    "\n",
    "class DoceeForInference(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            use_title: bool = False,\n",
    "            concat: Optional[Callable[[Iterable[str]], str]] = concat_dot_join\n",
    "    ):\n",
    "        columns = [\"title\", \"text\"] if use_title else [\"text\"]\n",
    "        self.concat = concat\n",
    "        self.df = df.loc[:, columns]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.concat(self.df.iloc[item])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "11370"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DoceeForInference(summary_df, use_title=False)\n",
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference loop:   0%|▏                                                                                                                                                          | 13/11370 [00:17<4:37:40,  1.47s/it]Your max_length is set to 156, but you input_length is only 154. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=77)\n",
      "Inference loop:   0%|▏                                                                                                                                                          | 13/11370 [00:17<4:13:42,  1.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/tmp/ipykernel_2454396/\u001B[0m\u001B[1;33m1768216289.py\u001B[0m:\u001B[94m3\u001B[0m in \u001B[92m<cell line: 3>\u001B[0m                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2454396/1768216289.py'\u001B[0m                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/tmp/ipykernel_2454396/\u001B[0m\u001B[1;33m1768216289.py\u001B[0m:\u001B[94m3\u001B[0m in \u001B[92m<listcomp>\u001B[0m                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2454396/1768216289.py'\u001B[0m                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tqdm/\u001B[0m\u001B[1;33ms\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[1;33mtd.py\u001B[0m:\u001B[94m1195\u001B[0m in \u001B[92m__iter__\u001B[0m                                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1192 \u001B[0m\u001B[2m│   │   \u001B[0mtime = \u001B[96mself\u001B[0m._time                                                                 \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1193 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1194 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mtry\u001B[0m:                                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1195 \u001B[2m│   │   │   \u001B[0m\u001B[94mfor\u001B[0m obj \u001B[95min\u001B[0m iterable:                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1196 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94myield\u001B[0m obj                                                                 \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1197 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[2m# Update and possibly print the progressbar.\u001B[0m                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1198 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[2m# Note: does not call self.update(1) for speed optimisation.\u001B[0m              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/pipelines/\u001B[0m\u001B[1;33mpt_utils.py\u001B[0m:\u001B[94m124\u001B[0m in \u001B[92m__next__\u001B[0m                                                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m121 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m \u001B[96mself\u001B[0m.loader_batch_item()                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m122 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m123 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# We're out of items within a batch\u001B[0m                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m124 \u001B[2m│   │   \u001B[0mitem = \u001B[96mnext\u001B[0m(\u001B[96mself\u001B[0m.iterator)                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m125 \u001B[0m\u001B[2m│   │   \u001B[0mprocessed = \u001B[96mself\u001B[0m.infer(item, **\u001B[96mself\u001B[0m.params)                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m126 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# We now have a batch of \"inferred things\".\u001B[0m                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m.loader_batch_size \u001B[95mis\u001B[0m \u001B[95mnot\u001B[0m \u001B[94mNone\u001B[0m:                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/pipelines/\u001B[0m\u001B[1;33mpt_utils.py\u001B[0m:\u001B[94m125\u001B[0m in \u001B[92m__next__\u001B[0m                                                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m122 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m123 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# We're out of items within a batch\u001B[0m                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m124 \u001B[0m\u001B[2m│   │   \u001B[0mitem = \u001B[96mnext\u001B[0m(\u001B[96mself\u001B[0m.iterator)                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m125 \u001B[2m│   │   \u001B[0mprocessed = \u001B[96mself\u001B[0m.infer(item, **\u001B[96mself\u001B[0m.params)                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m126 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# We now have a batch of \"inferred things\".\u001B[0m                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m.loader_batch_size \u001B[95mis\u001B[0m \u001B[95mnot\u001B[0m \u001B[94mNone\u001B[0m:                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m128 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[2m# Try to infer the size of the batch\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/pipelines/\u001B[0m\u001B[1;33mbase.py\u001B[0m:\u001B[94m990\u001B[0m in \u001B[92mforward\u001B[0m                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 987 \u001B[0m\u001B[2m│   │   │   │   \u001B[0minference_context = \u001B[96mself\u001B[0m.get_inference_context()                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 988 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mwith\u001B[0m inference_context():                                                 \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 989 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mmodel_inputs = \u001B[96mself\u001B[0m._ensure_tensor_on_device(model_inputs, device=\u001B[96mse\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 990 \u001B[2m│   │   │   │   │   \u001B[0mmodel_outputs = \u001B[96mself\u001B[0m._forward(model_inputs, **forward_params)         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 991 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mmodel_outputs = \u001B[96mself\u001B[0m._ensure_tensor_on_device(model_outputs, device=  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 992 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94melse\u001B[0m:                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 993 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mraise\u001B[0m \u001B[96mValueError\u001B[0m(\u001B[33mf\u001B[0m\u001B[33m\"\u001B[0m\u001B[33mFramework \u001B[0m\u001B[33m{\u001B[0m\u001B[96mself\u001B[0m.framework\u001B[33m}\u001B[0m\u001B[33m is not supported\u001B[0m\u001B[33m\"\u001B[0m)          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/pipelines/\u001B[0m\u001B[1;33mtext2text_generation.py\u001B[0m:\u001B[94m187\u001B[0m in \u001B[92m_forward\u001B[0m                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m184 \u001B[0m\u001B[2m│   │   \u001B[0mgenerate_kwargs[\u001B[33m\"\u001B[0m\u001B[33mmin_length\u001B[0m\u001B[33m\"\u001B[0m] = generate_kwargs.get(\u001B[33m\"\u001B[0m\u001B[33mmin_length\u001B[0m\u001B[33m\"\u001B[0m, \u001B[96mself\u001B[0m.model.con   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m185 \u001B[0m\u001B[2m│   │   \u001B[0mgenerate_kwargs[\u001B[33m\"\u001B[0m\u001B[33mmax_length\u001B[0m\u001B[33m\"\u001B[0m] = generate_kwargs.get(\u001B[33m\"\u001B[0m\u001B[33mmax_length\u001B[0m\u001B[33m\"\u001B[0m, \u001B[96mself\u001B[0m.model.con   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m186 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[96mself\u001B[0m.check_inputs(input_length, generate_kwargs[\u001B[33m\"\u001B[0m\u001B[33mmin_length\u001B[0m\u001B[33m\"\u001B[0m], generate_kwargs[\u001B[33m\"\u001B[0m   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m187 \u001B[2m│   │   \u001B[0moutput_ids = \u001B[96mself\u001B[0m.model.generate(**model_inputs, **generate_kwargs)                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m188 \u001B[0m\u001B[2m│   │   \u001B[0mout_b = output_ids.shape[\u001B[94m0\u001B[0m]                                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m189 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m.framework == \u001B[33m\"\u001B[0m\u001B[33mpt\u001B[0m\u001B[33m\"\u001B[0m:                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m190 \u001B[0m\u001B[2m│   │   │   \u001B[0moutput_ids = output_ids.reshape(in_b, out_b // in_b, *output_ids.shape[\u001B[94m1\u001B[0m:])    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mautograd/\u001B[0m\u001B[1;33mgrad_mode.py\u001B[0m:\u001B[94m27\u001B[0m in \u001B[92mdecorate_context\u001B[0m                                                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 24 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[1;95m@functools\u001B[0m.wraps(func)                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 25 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mdecorate_context\u001B[0m(*args, **kwargs):                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 26 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94mwith\u001B[0m \u001B[96mself\u001B[0m.clone():                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 27 \u001B[2m│   │   │   │   \u001B[0m\u001B[94mreturn\u001B[0m func(*args, **kwargs)                                               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 28 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mreturn\u001B[0m cast(F, decorate_context)                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 29 \u001B[0m\u001B[2m│   \u001B[0m                                                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 30 \u001B[0m\u001B[2m│   \u001B[0m\u001B[94mdef\u001B[0m \u001B[92m_wrap_generator\u001B[0m(\u001B[96mself\u001B[0m, func):                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/generation/\u001B[0m\u001B[1;33mutils.py\u001B[0m:\u001B[94m1518\u001B[0m in \u001B[92mgenerate\u001B[0m                                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1515 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m)                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1516 \u001B[0m\u001B[2m│   │   │   \u001B[0m                                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1517 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[2m# 10. run greedy search\u001B[0m                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1518 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m \u001B[96mself\u001B[0m.greedy_search(                                                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1519 \u001B[0m\u001B[2m│   │   │   │   \u001B[0minput_ids,                                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1520 \u001B[0m\u001B[2m│   │   │   │   \u001B[0mlogits_processor=logits_processor,                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1521 \u001B[0m\u001B[2m│   │   │   │   \u001B[0mstopping_criteria=stopping_criteria,                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/generation/\u001B[0m\u001B[1;33mutils.py\u001B[0m:\u001B[94m2285\u001B[0m in \u001B[92mgreedy_search\u001B[0m                                                 \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m2282 \u001B[0m\u001B[2m│   │   │   \u001B[0mmodel_inputs = \u001B[96mself\u001B[0m.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m2283 \u001B[0m\u001B[2m│   │   │   \u001B[0m                                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m2284 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[2m# forward pass to get next token\u001B[0m                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m2285 \u001B[2m│   │   │   \u001B[0moutputs = \u001B[96mself\u001B[0m(                                                               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m2286 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m**model_inputs,                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m2287 \u001B[0m\u001B[2m│   │   │   │   \u001B[0mreturn_dict=\u001B[94mTrue\u001B[0m,                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m2288 \u001B[0m\u001B[2m│   │   │   │   \u001B[0moutput_attentions=output_attentions,                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/models/bart/\u001B[0m\u001B[1;33mmodeling_bart.py\u001B[0m:\u001B[94m1369\u001B[0m in \u001B[92mforward\u001B[0m                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1366 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mlabels, \u001B[96mself\u001B[0m.config.pad_token_id, \u001B[96mself\u001B[0m.config.decoder_start_token_id  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1367 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m)                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1368 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1369 \u001B[2m│   │   \u001B[0moutputs = \u001B[96mself\u001B[0m.model(                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1370 \u001B[0m\u001B[2m│   │   │   \u001B[0minput_ids,                                                                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1371 \u001B[0m\u001B[2m│   │   │   \u001B[0mattention_mask=attention_mask,                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1372 \u001B[0m\u001B[2m│   │   │   \u001B[0mdecoder_input_ids=decoder_input_ids,                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/models/bart/\u001B[0m\u001B[1;33mmodeling_bart.py\u001B[0m:\u001B[94m1251\u001B[0m in \u001B[92mforward\u001B[0m                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1248 \u001B[0m\u001B[2m│   │   │   \u001B[0m)                                                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1249 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1250 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_att\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1251 \u001B[2m│   │   \u001B[0mdecoder_outputs = \u001B[96mself\u001B[0m.decoder(                                                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1252 \u001B[0m\u001B[2m│   │   │   \u001B[0minput_ids=decoder_input_ids,                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1253 \u001B[0m\u001B[2m│   │   │   \u001B[0mattention_mask=decoder_attention_mask,                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1254 \u001B[0m\u001B[2m│   │   │   \u001B[0mencoder_hidden_states=encoder_outputs[\u001B[94m0\u001B[0m],                                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/models/bart/\u001B[0m\u001B[1;33mmodeling_bart.py\u001B[0m:\u001B[94m1107\u001B[0m in \u001B[92mforward\u001B[0m                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1104 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m)                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1105 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94melse\u001B[0m:                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1106 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m                                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1107 \u001B[2m│   │   │   │   \u001B[0mlayer_outputs = decoder_layer(                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1108 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mhidden_states,                                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1109 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mattention_mask=attention_mask,                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1110 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mencoder_hidden_states=encoder_hidden_states,                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mormers/models/bart/\u001B[0m\u001B[1;33mmodeling_bart.py\u001B[0m:\u001B[94m456\u001B[0m in \u001B[92mforward\u001B[0m                                               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 453 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 454 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Fully Connected\u001B[0m                                                                 \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 455 \u001B[0m\u001B[2m│   │   \u001B[0mresidual = hidden_states                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 456 \u001B[2m│   │   \u001B[0mhidden_states = \u001B[96mself\u001B[0m.activation_fn(\u001B[96mself\u001B[0m.fc1(hidden_states))                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 457 \u001B[0m\u001B[2m│   │   \u001B[0mhidden_states = nn.functional.dropout(hidden_states, p=\u001B[96mself\u001B[0m.activation_dropout,   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 458 \u001B[0m\u001B[2m│   │   \u001B[0mhidden_states = \u001B[96mself\u001B[0m.fc2(hidden_states)                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 459 \u001B[0m\u001B[2m│   │   \u001B[0mhidden_states = nn.functional.dropout(hidden_states, p=\u001B[96mself\u001B[0m.dropout, training=\u001B[96mse\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnn/modules/\u001B[0m\u001B[1;33mmodule.py\u001B[0m:\u001B[94m1130\u001B[0m in \u001B[92m_call_impl\u001B[0m                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1127 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# this function, and just call forward.\u001B[0m                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1128 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[95mnot\u001B[0m (\u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_hooks \u001B[95mor\u001B[0m \u001B[96mself\u001B[0m._forward_pre_hooks \u001B[95mo\u001B[0m  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1129 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[95mor\u001B[0m _global_forward_hooks \u001B[95mor\u001B[0m _global_forward_pre_hooks):                   \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1130 \u001B[2m│   │   │   \u001B[0m\u001B[94mreturn\u001B[0m forward_call(*\u001B[96minput\u001B[0m, **kwargs)                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1131 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# Do not call functions when jit is used\u001B[0m                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1132 \u001B[0m\u001B[2m│   │   \u001B[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m1133 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mself\u001B[0m._backward_hooks \u001B[95mor\u001B[0m _global_backward_hooks:                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33m/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/\u001B[0m \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[2;33mnn/modules/\u001B[0m\u001B[1;33mlinear.py\u001B[0m:\u001B[94m114\u001B[0m in \u001B[92mforward\u001B[0m                                                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m111 \u001B[0m\u001B[2m│   │   │   \u001B[0minit.uniform_(\u001B[96mself\u001B[0m.bias, -bound, bound)                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m112 \u001B[0m\u001B[2m│   \u001B[0m                                                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m113 \u001B[0m\u001B[2m│   \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mforward\u001B[0m(\u001B[96mself\u001B[0m, \u001B[96minput\u001B[0m: Tensor) -> Tensor:                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m114 \u001B[2m│   │   \u001B[0m\u001B[94mreturn\u001B[0m F.linear(\u001B[96minput\u001B[0m, \u001B[96mself\u001B[0m.weight, \u001B[96mself\u001B[0m.bias)                                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m115 \u001B[0m\u001B[2m│   \u001B[0m                                                                                       \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m116 \u001B[0m\u001B[2m│   \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mextra_repr\u001B[0m(\u001B[96mself\u001B[0m) -> \u001B[96mstr\u001B[0m:                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m117 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mreturn\u001B[0m \u001B[33m'\u001B[0m\u001B[33min_features=\u001B[0m\u001B[33m{}\u001B[0m\u001B[33m, out_features=\u001B[0m\u001B[33m{}\u001B[0m\u001B[33m, bias=\u001B[0m\u001B[33m{}\u001B[0m\u001B[33m'\u001B[0m.format(                          \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mKeyboardInterrupt\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2454396/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1768216289.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 3&gt;</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2454396/1768216289.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2454396/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1768216289.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2454396/1768216289.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/tqdm/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">s</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">td.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1195</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__iter__</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span>time = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._time                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1194 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1195 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> obj <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> iterable:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">yield</span> obj                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   │   │   # Update and possibly print the progressbar.</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1198 │   │   │   │   # Note: does not call self.update(1) for speed optimisation.</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pt_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">124</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__next__</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.loader_batch_item()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 │   │   # We're out of items within a batch</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>124 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>item = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.iterator)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 │   │   </span>processed = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.infer(item, **<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.params)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 │   │   # We now have a batch of \"inferred things\".</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.loader_batch_size <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pt_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">125</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__next__</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 │   │   # We're out of items within a batch</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 │   │   </span>item = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.iterator)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>125 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>processed = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.infer(item, **<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.params)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 │   │   # We now have a batch of \"inferred things\".</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.loader_batch_size <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">128 │   │   │   # Try to infer the size of the batch</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">990</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 987 │   │   │   │   </span>inference_context = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_inference_context()                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 988 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> inference_context():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 989 │   │   │   │   │   </span>model_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._ensure_tensor_on_device(model_inputs, device=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 990 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>model_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward(model_inputs, **forward_params)         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 991 │   │   │   │   │   </span>model_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._ensure_tensor_on_device(model_outputs, device=  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 992 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 993 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Framework {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.framework<span style=\"color: #808000; text-decoration-color: #808000\">} is not supported\"</span>)          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">text2text_generation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">187</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_forward</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184 │   │   </span>generate_kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"min_length\"</span>] = generate_kwargs.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"min_length\"</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.con   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">185 │   │   </span>generate_kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"max_length\"</span>] = generate_kwargs.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"max_length\"</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.con   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">186 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.check_inputs(input_length, generate_kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"min_length\"</span>], generate_kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>187 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output_ids = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.generate(**model_inputs, **generate_kwargs)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188 │   │   </span>out_b = output_ids.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.framework == <span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">190 │   │   │   </span>output_ids = output_ids.reshape(in_b, out_b // in_b, *output_ids.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:])    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_mode.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">27</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.clone():                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 27 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 28 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> cast(F, decorate_context)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 29 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 30 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrap_generator</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, func):                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1518</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1515 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1516 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1517 │   │   │   # 10. run greedy search</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1518 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.greedy_search(                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519 │   │   │   │   </span>input_ids,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1520 │   │   │   │   </span>logits_processor=logits_processor,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1521 │   │   │   │   </span>stopping_criteria=stopping_criteria,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2285</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">greedy_search</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2282 │   │   │   </span>model_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare_inputs_for_generation(input_ids, **model_kwargs)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2283 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2284 │   │   │   # forward pass to get next token</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2285 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2286 │   │   │   │   </span>**model_inputs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2287 │   │   │   │   </span>return_dict=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2288 │   │   │   │   </span>output_attentions=output_attentions,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1369</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1366 │   │   │   │   │   </span>labels, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.pad_token_id, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.decoder_start_token_id  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1367 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1368 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1369 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1370 │   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1371 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1372 │   │   │   </span>decoder_input_ids=decoder_input_ids,                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1251</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1248 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1249 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1250 │   │   # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_att</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1251 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>decoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1252 │   │   │   </span>input_ids=decoder_input_ids,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1253 │   │   │   </span>attention_mask=decoder_attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1254 │   │   │   </span>encoder_hidden_states=encoder_outputs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>],                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1107</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1106 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1107 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>layer_outputs = decoder_layer(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1108 │   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1109 │   │   │   │   │   </span>attention_mask=attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1110 │   │   │   │   │   </span>encoder_hidden_states=encoder_hidden_states,                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/transf</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ormers/models/bart/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bart.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">456</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 453 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 454 │   │   # Fully Connected</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 455 │   │   </span>residual = hidden_states                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 456 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.activation_fn(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fc1(hidden_states))                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 457 │   │   </span>hidden_states = nn.functional.dropout(hidden_states, p=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.activation_dropout,   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 458 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fc2(hidden_states)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 459 │   │   </span>hidden_states = nn.functional.dropout(hidden_states, p=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout, training=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jvidakovic/.conda/envs/cross_lingual_data_augmentation/lib/python3.10/site-packages/torch/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   │   </span>init.uniform_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, -bound, bound)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor) -&gt; Tensor:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">extra_repr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #808000; text-decoration-color: #808000\">'in_features={}, out_features={}, bias={}'</span>.format(                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "summary_df.loc[:, [\"text\", \"source_doc_id\"]] = [\n",
    "    (out[j][\"summary_text\"], source_doc_ids[i])\n",
    "    for i, out in enumerate(tqdm(summarizer(\n",
    "        dataset,\n",
    "        truncation=True,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        min_length=20,\n",
    "        max_length=156,\n",
    "        num_beams=1,\n",
    "        early_stopping=True,\n",
    "        top_k=0,\n",
    "        top_p=1.0,\n",
    "        temperature=1.0,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        penalty_alpha=0,\n",
    "    ), desc=f\"Inference loop\", total=len(dataset)))\n",
    "    for j in range(len(out))\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "11931"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[8])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[855, 3373, 2692, 1782, 1675, 1027, 4195, 2783]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(dataset[i]) for i in range(8)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "'text'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:8]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
